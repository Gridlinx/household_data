{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fdd7071a-0bcf-48c5-9ed7-772ab69e59df"
    }
   },
   "source": [
    "<table style=\"width:100%; background-color: #EBF5FB\">\n",
    "  <tr>\n",
    "    <td style=\"border: 1px solid #CFCFCF\">\n",
    "      <b>Household data: Processing Notebook</b>\n",
    "      <ul>\n",
    "        <li><a href=\"main.ipynb\">Main Notebook</a></li>\n",
    "        <li>Processing Notebook</li>\n",
    "      </ul>\n",
    "      <br>This Notebook is part of the <a href=\"http://data.open-power-system-data.org/household_data\">Household Data Package</a> of <a href=\"http://open-power-system-data.org\">Open Power System Data</a>.\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1. Introductory Notes](#1.-Introductory-Notes)\n",
    "* [2. Settings](#2.-Settings)\n",
    "\t* [2.1 Set version number and recent changes](#2.1-Set-version-number-and-recent-changes)\n",
    "\t* [2.2 Import Python libraries](#2.2-Import-Python-libraries)\n",
    "\t* [2.3 Set directories](#2.3-Set-directories)\n",
    "\t* [2.4 Set up a log](#2.4-Set-up-a-log)\n",
    "\t* [2.5 Select timerange](#2.5-Select-timerange)\n",
    "\t* [2.6 Select download source](#2.6-Select-download-source)\n",
    "\t* [2.7 Select household subset](#2.7-Select-household-subset)\n",
    "* [3. Download](#3.-Download)\n",
    "* [4. Read](#4.-Read)\n",
    "\t* [4.1 Preparations](#4.1-Preparations)\n",
    "\t* [4.2 Reading loop](#4.2-Reading-loop)\n",
    "\t* [4.3 Save raw data](#4.3-Save-raw-data)\n",
    "* [5. Processing](#5.-Processing)\n",
    "\t* [5.1 Validate Data](#5.1-Validate-Data)\n",
    "    * [5.2 Aggregate Index equidistantly](#5.2-Aggregate-Index-equidistantly)\n",
    "    * [5.3 Missing Data Handling](#5.3-Missing-Data-Handling)\n",
    "    * [5.4 Aggregate hourly and 15min interval data](#5.4-Aggregate-hourly-and-15min-interval-data)\n",
    "    * [5.5 Insert a column with Central European Time](#5.5-Insert-a-column-with-Central-European-Time)\n",
    "* [6. Create metadata](#6.-Create-metadata)\n",
    "* [7. Write data to disk](#7.-Write-data-to-disk)\n",
    "\t* [7.1 Limit time range](#7.1-Limit-time-range)\n",
    "    * [7.2 Convert the Central European Time column to ISO-8601](#7.2-Convert-the-Central-European-Time-column-to-ISO-8601)\n",
    "    * [7.3 Different shapes](#7.3-Different-shapes)\n",
    "    * [7.4 Write to SQL-database](#7.4-Write-to-SQL-database)\n",
    "    * [7.5 Write to Excel](#7.5-Write-to-Excel)\n",
    "    * [7.6 Write to CSV](#7.6-Write-to-CSV)\n",
    "    * [7.7 Write checksums.txt](#7.7-Write-checksums.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ba6b62da-6cee-476b-a563-c945f3fd0f79"
    }
   },
   "source": [
    "# 1. Introductory Notes\n",
    "\n",
    "This Notebook handles missing data, performs calculations and aggragations and creates the output files.\n",
    "\n",
    "# 2. Settings\n",
    "\n",
    "## 2.1 Set version number and recent changes\n",
    "Executing this script till the end will create a new version of the data package.\n",
    "The Version number specifies the local directory for the data <br>\n",
    "We include a note on what has been changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = '2020-04-15'\n",
    "changes = 'Second release of all CoSSMic households'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2b838df4-f987-4ae4-a132-9c898e3ffab1"
    }
   },
   "source": [
    "## 2.2 Import Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true,
    "nbpresent": {
     "id": "c0035fc6-ff1d-44d8-a3fd-b4c08f53be71"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Python modules\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import yaml\n",
    "import sqlite3\n",
    "import itertools\n",
    "import hashlib\n",
    "import pytz\n",
    "from shutil import copyfile\n",
    "from datetime import datetime, date, timedelta, time\n",
    "\n",
    "# Reload modules with execution of any code, to avoid having to restart\n",
    "# the kernel after editing timeseries_scripts\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Set directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the working directory is this file's directory\n",
    "try:\n",
    "    os.chdir(home_path)\n",
    "except NameError:\n",
    "    home_path = os.getcwd()\n",
    "\n",
    "config_path = os.path.join(home_path, 'conf')\n",
    "data_path = os.path.join(home_path, 'household_data', version, 'original_data')\n",
    "out_path = os.path.join(home_path, 'household_data', version)\n",
    "temp_path = os.path.join(home_path, 'household_data', 'temp')\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "os.makedirs(temp_path, exist_ok=True)\n",
    "os.chdir(temp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Setup logging and scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging.config\n",
    "\n",
    "logging_file = os.path.join(config_path, 'logging.cfg')\n",
    "logging.config.fileConfig(logging_file)\n",
    "\n",
    "# Scripts from household repository package\n",
    "from household.download import download\n",
    "from household.read import read\n",
    "from household.tools import update_sets\n",
    "from household.validation import validate\n",
    "from household.visualization import visualize\n",
    "from household.imputation import make_equidistant, fill_nan, resample_markers\n",
    "from household.make_json import make_json\n",
    "\n",
    "# Additional verbosity like printing out additional CSV files to verify feed integrity\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Select timerange\n",
    "\n",
    "Select the time range to read and process data. <br>\n",
    "*Default: all data.*\n",
    "\n",
    "Type `None` to process all available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_from_user = None  # i.e. date(2017, 1, 1)\n",
    "end_from_user = None  # i.e. date(2017, 1, 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Select download source\n",
    "\n",
    "The raw data can be downloaded as a zip file from the OPSD Server. To do this, specify an archive version to use, that has been cached on the OPSD server as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_version = '2020-04-15'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Select household subset\n",
    "\n",
    "Optionally, specify a subset of households to process. <br>\n",
    "The next cell prints the available sources and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(config_path, 'households.yml'), 'r') as f:\n",
    "    households = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "for household_name, household_data in households.items():\n",
    "    household_id = household_name.replace(' ', '').lower()\n",
    "    household_data['id'] = household_id\n",
    "    household_data['name'] = household_name\n",
    "    print(yaml.dump({household_name: list(household_data['series'].keys())}, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy from its output and paste to following cell to get the right format.\n",
    "\n",
    "Type `subset = None` to ignore any subset selection and include all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = yaml.load('''\n",
    "Example household:\n",
    "- example_series\n",
    "''', Loader=yaml.FullLoader)\n",
    "\n",
    "subset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now eliminate households and feeds not in subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if subset:  # eliminate households and feeds not in subset\n",
    "    households = {household_name: {k: v for k, v in households[household_name].items()}\n",
    "                  for household_name, series_list in subset.items()}\n",
    "    \n",
    "    for household_name in subset.keys():\n",
    "        series_ignore = []\n",
    "        for series_name in households[household_name]['series']:\n",
    "            if not series_name in subset[household_name]:\n",
    "                series_ignore.append(series_name)\n",
    "    \n",
    "    for series_name in series_ignore:\n",
    "        del households[household_name]['series'][series_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Download\n",
    "\n",
    "This section: download raw data to process.\n",
    "\n",
    "If the original data does not exist, it will be downloaded from the OPSD Server and extracted in a local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "row",
    "nbpresent": {
     "id": "0c1eb987-6d5f-4e3d-9248-df80b9f37a49"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "download(out_path, version=archive_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d0b353e7-179d-4556-bdd2-270192c830fb"
    }
   },
   "source": [
    "# 4. Read\n",
    "\n",
    "This section: Read each downloaded file into a pandas-DataFrame and merge data from different sources if it has the same time resolution.\n",
    "\n",
    "## 4.1 Preparations\n",
    "Set the title of the rows at the top of the data used to store metadata internally. The order of this list determines the order of the levels in the resulting output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "row",
    "init_cell": true,
    "nbpresent": {
     "id": "4dc92cc3-c01d-4c83-9252-80958edbe0f9"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "headers = ['region', 'household', 'type', 'unit', 'feed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Reading loop\n",
    "\n",
    "Loop through households and feeds to do the reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For each source in the household dictionary\n",
    "household_data = {}\n",
    "for household in households.values():\n",
    "    data = read(household['name'], household['dir'], household['region'], household['type'], \n",
    "                household['series'], headers, \n",
    "                start_from_user=start_from_user,\n",
    "                end_from_user=end_from_user)\n",
    "    \n",
    "    household_data[household['id']] = data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Validate and save raw data\n",
    "\n",
    "With the raw data being the unvalidated measurements of a research prototype under development, certain steps should be taken, to remove clearly incorrect measured data points or react to other events, such as the counter reset of an energy meter.\n",
    "\n",
    "Save the validated DataFrames to disk. This way you have the raw data to fall back to if something goes wrong in the remainder of this notebook without having to repeat the previos steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "os.makedirs('raw_data', exist_ok=True)\n",
    "for household in households.values():\n",
    "    data = validate(household, household_data[household['id']], config_dir=config_path, verbose=verbose)\n",
    "    data.columns.names = headers\n",
    "    data.to_pickle(os.path.join('raw_data', household['id']+'.pickle'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data series saved above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_data = {}\n",
    "for household in households.values():\n",
    "    household_data[household['id']] = pd.read_pickle(os.path.join('raw_data', household['id']+'.pickle'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Processing\n",
    "\n",
    "This section: Handle missing data and aggregation to a regular, equidistant index, as well as 15 and 60 minute intervals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5.1 Aggregate Index equidistantly\n",
    "\n",
    "Most of the acquired household data comes in irregular time intervals and is highly unsynchronized for different data series.\n",
    "\n",
    "To improve readability and comparability, regular intervals will be assumed and interpolated according to existing values. Gaps greater than 15 minutes will be left untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('fixed_data', exist_ok=True)\n",
    "for household in households.values():\n",
    "    data = make_equidistant(household, household_data[household['id']], 1)\n",
    "    data.to_pickle(os.path.join('fixed_data', household['id']+'.pickle'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the equidistant data series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_data = {}\n",
    "for household in households.values():\n",
    "    household_data[household['id']] = pd.read_pickle(os.path.join('fixed_data', household['id']+'.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Missing Data Handling\n",
    "\n",
    "Patch missing data. At this stage, only small gaps (up to 1 hour) are filled by linear interpolation. This catched most of the missing data due to daylight savings time transitions or failed radio transmissions, while filling bigger gaps with values from the prior day or prior week.\n",
    "\n",
    "The exact locations of missing data are stored in the `data_nan` DataFrames.\n",
    "\n",
    "Where data has been interpolated, it is marked in a new column `comment`. For eaxample the comment `residential_004_pv;` means that in the original data, there is a gap in the solar generation timeseries from the Resident 4 in the time period where the marker appears.\n",
    "\n",
    "Patch the datasets and display the location of missing Data in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('filled_data', exist_ok=True)\n",
    "for household in households.values():\n",
    "    data, data_nan = fill_nan(household_data[household['id']], household['name'], headers, config_dir=config_path)\n",
    "    data.to_pickle(os.path.join('filled_data', household['id']+'.pickle'))\n",
    "    \n",
    "    writer = pd.ExcelWriter(os.path.join('filled_data', household['id']+'_NaN.xlsx'))\n",
    "    data_nan.to_excel(writer, 'NaN')\n",
    "    writer.save()\n",
    "    \n",
    "    if verbose:\n",
    "        visualize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all patched data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = {}\n",
    "with open(os.path.join(config_path, 'households.yml'), 'r') as f:\n",
    "    households_full = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "\n",
    "for household_name in households_full:\n",
    "    household_id = household_name.replace(' ', '').lower()\n",
    "\n",
    "    #data_file = os.path.join('raw_data', household_id+'.pickle')\n",
    "    #if os.path.isfile(data_file):\n",
    "    #    data = pd.read_pickle(data_file)\n",
    "    #    update_sets('raw', data, data_sets)\n",
    "\n",
    "    data_file = os.path.join('filled_data', household_id+'.pickle')\n",
    "    if os.path.isfile(data_file):\n",
    "        data = pd.read_pickle(data_file)\n",
    "        update_sets('1min', data, data_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5.3 Aggregate hourly and 15min interval data\n",
    "\n",
    "As some data comes in 1-minute, other (older series) in 3-minute intervals, a harmonization can be done. With most billing intervals being at 15-minute and 60-minute ranges, a resampling to those intervals can be done to improve the overview over the data.\n",
    "\n",
    "The marker column is resampled separately in such a way that all information on where data has been interpolated is preserved.\n",
    "\n",
    "To do this, condense the marker column to both 15 and 60 minutes resolution, resample the series and update the condensed markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_sets['1min']\n",
    "minute = data.index[0].minute + (15 - data.index[0].minute % 15)\n",
    "hour = data.index[0].hour\n",
    "if(minute > 59):\n",
    "    minute = 0\n",
    "    hour += 1\n",
    "start_15 = data.index[0].replace(hour=hour, minute=minute, second=0)\n",
    "\n",
    "minute = data.index[-1].minute + (15 - data.index[-1].minute % 15)\n",
    "hour = data.index[-1].hour\n",
    "if(minute > 59):\n",
    "    minute = 0\n",
    "    hour += 1\n",
    "end_15 = data.index[-1].replace(hour=hour, minute=minute, second=0)\n",
    "\n",
    "index_15 = pd.date_range(start=start_15, end=end_15, freq='15min')\n",
    "marker_15 = data['interpolated'].groupby(\n",
    "    pd.Grouper(freq='15min', closed='left', label='left')\n",
    "    ).agg(resample_markers).reindex(index_15)\n",
    "\n",
    "data_sets['15min'] = data.resample('15min').last()\n",
    "data_sets['15min']['interpolated'] = marker_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_60 = data.index[0].replace(minute=0, second=0)\n",
    "end_60 = data.index[-1].replace(minute=0, second=0)\n",
    "index_60 = pd.date_range(start=start_60, end=end_60, freq='60min')\n",
    "marker_60 = data['interpolated'].groupby(\n",
    "    pd.Grouper(freq='60min', closed='left', label='left')\n",
    "    ).agg(resample_markers).reindex(index_60)\n",
    "\n",
    "data_sets['60min'] = data.resample('60min').last()\n",
    "data_sets['60min']['interpolated'] = marker_60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Insert a column with Central European Time\n",
    "\n",
    "The index column of th data sets defines the start of the timeperiod represented by each row of that data set in **UTC** time. We include an additional column for the **CE(S)T** Central European (Summer-) Time, as this might help aligning the output data with other data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_cols = {'utc': 'utc_timestamp',\n",
    "             'cet': 'cet_cest_timestamp',\n",
    "             'marker': 'interpolated'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res_key, df in data_sets.items():\n",
    "    if df.empty:\n",
    "        continue\n",
    "    if df.index.tzinfo is None or df.index.tzinfo.utcoffset(df.index) is None:\n",
    "        df.index = df.index.tz_localize('UTC')\n",
    "    df.index.rename(info_cols['utc'], inplace=True)\n",
    "    df.insert(0, info_cols['cet'], df.index.tz_convert('Europe/Berlin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a final savepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('final_data', exist_ok=True)\n",
    "for res_key, data_set in data_sets.items():\n",
    "    data_set.to_pickle(os.path.join('final_data', res_key+'.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = {}\n",
    "#data_sets['raw'] = pd.read_pickle(os.path.join('final_data', 'raw.pickle'))\n",
    "data_sets['1min'] = pd.read_pickle(os.path.join('final_data', '1min.pickle'))\n",
    "data_sets['15min'] = pd.read_pickle(os.path.join('final_data', '15min.pickle'))\n",
    "data_sets['60min'] = pd.read_pickle(os.path.join('final_data', '60min.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 6. Create metadata\n",
    "\n",
    "This section: create the metadata, both general and column-specific. All metadata we be stored as a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to out_path directory\n",
    "os.chdir(out_path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_json(data_sets, info_cols, version, changes, headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Write data to disk\n",
    "\n",
    "This section: Save as [Data Package](http://data.okfn.org/doc/tabular-data-package) (data in CSV, metadata in JSON file). All files are saved in the directory of this notebook. Alternative file formats (SQL, XLSX) are also exported. Takes about 1 hour to run.\n",
    "\n",
    "## 7.1 Limit time range\n",
    "\n",
    "Cut off the data outside of [start_from_user:end_from_user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res_key, df in data_sets.items():\n",
    "    # First, convert userinput to UTC time to conform with data_set.index\n",
    "    if start_from_user:\n",
    "        start_from_user = (\n",
    "            pytz.timezone('Europe/Berlin')\n",
    "            .localize(datetime.combine(start_from_user, time()))\n",
    "            .astimezone(pytz.timezone('UTC')))\n",
    "    if end_from_user and 'min' in res_key:\n",
    "        end_from_user = (\n",
    "            pytz.timezone('Europe/Berlin')\n",
    "            .localize(datetime.combine(end_from_user, time()))\n",
    "            .astimezone(pytz.timezone('UTC'))\n",
    "            # appropriate offset to inlude the end of period\n",
    "            + timedelta(days=1, minutes=-int(res_key[:res_key.index('min')])))\n",
    "    # Then cut off the data_set\n",
    "    data_sets[res_key] = df.loc[start_from_user:end_from_user, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Convert the Central European Time column to ISO-8601\n",
    "\n",
    "Convert the UTC, as well as the Central European Time indexes to ISO-8601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res_key, df in data_sets.items():\n",
    "    df.iloc[:,0] = df.iloc[:,0].dt.strftime('%Y-%m-%dT%H:%M:%S%z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Different shapes\n",
    "\n",
    "Data are provided in three different \"shapes\": \n",
    "- SingleIndex (easy to read for humans, compatible with datapackage standard, small file size)\n",
    "  - Fileformat: CSV, SQLite\n",
    "- MultiIndex (easy to read into GAMS, not compatible with datapackage standard, small file size)\n",
    "  - Fileformat: CSV, Excel\n",
    "- Stacked (compatible with data package standard, large file size, many rows, too many for Excel) \n",
    "  - Fileformat: CSV\n",
    "\n",
    "The different shapes need to be created internally befor they can be saved to files. Takes about 1 minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets_singleindex = {}\n",
    "data_sets_multiindex = {}\n",
    "data_sets_stacked = {}\n",
    "for res_key, df in data_sets.items():\n",
    "    # MultIndex\n",
    "    data_sets_multiindex[res_key + '_multiindex'] = df\n",
    "    \n",
    "    # SingleIndex\n",
    "    df_singleindex = df.copy()\n",
    "    # use first 3 levels of multiindex to create singleindex\n",
    "    df_singleindex.columns = [\n",
    "        col[0] if col[0] in info_cols.values()\n",
    "        else next(iter([l for l in col if l in df.columns.get_level_values('region')] or []), None) + \\\n",
    "                '_' + next(iter([l for l in col if l in df.columns.get_level_values('household')] or []), None) + \\\n",
    "                '_' + next(iter([l for l in col if l in df.columns.get_level_values('feed')] or []), None)\n",
    "        for col in df.columns.values]\n",
    "    \n",
    "    data_sets_singleindex[res_key + '_singleindex'] = df_singleindex\n",
    "    \n",
    "    # Stacked\n",
    "    stacked = df.copy()\n",
    "    stacked.drop(info_cols['cet'], axis=1, inplace=True)\n",
    "    stacked.columns = stacked.columns.droplevel(['region', 'type', 'unit'])\n",
    "    stacked = stacked.transpose().stack(dropna=True).to_frame(name='data')\n",
    "    data_sets_stacked[res_key + '_stacked'] = stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Write to SQL-database\n",
    "\n",
    "This file format is required for the filtering function on the OPSD website. This takes about 30 seconds to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res_key, df in data_sets_singleindex.items():\n",
    "    if res_key.startswith('raw'):\n",
    "        continue\n",
    "    \n",
    "    df_sql = df.copy()\n",
    "    df_sql.index = df_sql.index.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    \n",
    "    table = 'household_data_' + res_key\n",
    "    df_sql.to_sql(table, sqlite3.connect('household_data.sqlite'),\n",
    "                  if_exists='replace', index_label=info_cols['utc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Write to Excel\n",
    "\n",
    "Writing the full tables to Excel takes extremely long. As a workaround, only the first 5 rows are exported. The rest of the data can than be inserted manually from the `_multindex.csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('household_data.xlsx')\n",
    "for res_key, df in data_sets_multiindex.items():\n",
    "    if res_key.startswith('raw') or res_key.startswith('1min'):\n",
    "        # Excel max sheet size is 1048576 rows, while raw and 1min resolution data has a lot more\n",
    "        continue\n",
    "    \n",
    "    df_csv = df.copy()\n",
    "    df_csv.index = df_csv.index.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    \n",
    "    df_csv.to_excel(writer, res_key.split('_')[0], float_format='%.3f',\n",
    "                    merge_cells=True)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itertoools.chain() allows iterating over multiple dicts at once\n",
    "for res_key, df in itertools.chain(\n",
    "        data_sets_singleindex.items(),\n",
    "        data_sets_multiindex.items(),\n",
    "        data_sets_stacked.items()\n",
    "    ):\n",
    "    filename = 'household_data_' + res_key + '.csv'\n",
    "    df.to_csv(filename, float_format='%.3f',\n",
    "              date_format='%Y-%m-%dT%H:%M:%SZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Write checksums.txt\n",
    "\n",
    "We publish SHA-checksums for the outputfiles on GitHub to allow verifying the integrity of outputfiles on the OPSD server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sha_hash(path, blocksize=65536):\n",
    "    sha_hasher = hashlib.sha256()\n",
    "    with open(path, 'rb') as f:\n",
    "        buffer = f.read(blocksize)\n",
    "        while len(buffer) > 0:\n",
    "            sha_hasher.update(buffer)\n",
    "            buffer = f.read(blocksize)\n",
    "        return sha_hasher.hexdigest()\n",
    "\n",
    "files = os.listdir(out_path)\n",
    "\n",
    "# Create checksums.txt in the output directory\n",
    "with open('checksums.txt', 'w') as f:\n",
    "    for file_name in files:\n",
    "        if file_name.split('.')[-1] in ['csv', 'sqlite', 'xlsx']:\n",
    "            file_hash = get_sha_hash(file_name)\n",
    "            f.write('{},{}\\n'.format(file_name, file_hash))\n",
    "\n",
    "# Copy the file to root directory from where it will be pushed to GitHub,\n",
    "# leaving a copy in the version directory for reference\n",
    "copyfile('checksums.txt', os.path.join(home_path, 'checksums.txt'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Initialisation Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "nbpresent": {
   "slides": {
    "f6b300bf-88b5-4dea-951e-c926a9ea8287": {
     "id": "f6b300bf-88b5-4dea-951e-c926a9ea8287",
     "prev": "f96dd4bc-93a6-4014-b85f-a43061cf5688",
     "regions": {
      "dc486e18-7547-4610-99c0-55dfb5553f62": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "c0035fc6-ff1d-44d8-a3fd-b4c08f53be71",
        "part": "source"
       },
       "id": "dc486e18-7547-4610-99c0-55dfb5553f62"
      }
     }
    },
    "f96dd4bc-93a6-4014-b85f-a43061cf5688": {
     "id": "f96dd4bc-93a6-4014-b85f-a43061cf5688",
     "prev": null,
     "regions": {
      "657c3ad3-2fcf-4c8e-a527-de3d0a46fa4e": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "1562965a-7d74-4c1c-8251-4d82847f294a",
        "part": "source"
       },
       "id": "657c3ad3-2fcf-4c8e-a527-de3d0a46fa4e"
      }
     }
    }
   },
   "themes": {}
  },
  "notify_time": "10"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
