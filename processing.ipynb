{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fdd7071a-0bcf-48c5-9ed7-772ab69e59df"
    }
   },
   "source": [
    "<table style=\"width:100%; background-color: #EBF5FB\">\n",
    "  <tr>\n",
    "    <td style=\"border: 1px solid #CFCFCF\">\n",
    "      <b>Household data: Processing Notebook</b>\n",
    "      <ul>\n",
    "        <li><a href=\"main.ipynb\">Main Notebook</a></li>\n",
    "        <li>Processing Notebook</li>\n",
    "      </ul>\n",
    "      <br>This Notebook is part of the <a href=\"http://data.open-power-system-data.org/household_data\">Household Data Package</a> of <a href=\"http://open-power-system-data.org\">Open Power System Data</a>.\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1. Introductory Notes](#1.-Introductory-Notes)\n",
    "* [2. Settings](#2.-Settings)\n",
    "\t* [2.1 Import Python libraries](#2.1-Import-Python-libraries)\n",
    "\t* [2.2 Set version number and recent changes](#2.2-Set-version-number-and-recent-changes)\n",
    "\t* [2.3 Select timerange](#2.3-Select-timerange)\n",
    "\t* [2.4 Select download source](#2.4-Select-download-source)\n",
    "* [3. Download](#3.-Download)\n",
    "* [4. Read](#4.-Read)\n",
    "\t* [4.1 Preparations](#4.1-Preparations)\n",
    "\t* [4.2 Select household subset](#4.2-Select-household-subset)\n",
    "\t* [4.3 Reading loop](#4.3-Reading-loop)\n",
    "\t* [4.4 Save raw data](#4.4-Save-raw-data)\n",
    "* [5. Processing](#5.-Processing)\n",
    "\t* [5.1 Validate Data](#5.1-Validate-Data)\n",
    "    * [5.2 Aggregate Index equidistantly](#5.2-Aggregate-Index-equidistantly)\n",
    "    * [5.3 Missing Data Handling](#5.3-Missing-Data-Handling)\n",
    "    * [5.4 Aggregate hourly and 15min interval data](#5.4-Aggregate-hourly-and-15min-interval-data)\n",
    "    * [5.5 Insert a column with Central European Time](#5.5-Insert-a-column-with-Central-European-Time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bba7a58e-f3b2-4d3b-9617-d734c369084f"
    }
   },
   "source": [
    "# 1. Introductory Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook handles missing data, performs calculations and aggragations and creates the output files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ba6b62da-6cee-476b-a563-c945f3fd0f79"
    }
   },
   "source": [
    "# 2. Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2b838df4-f987-4ae4-a132-9c898e3ffab1"
    }
   },
   "source": [
    "## 2.1 Import Python libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d4e82608-1ad1-4c51-8285-929a5a92c5b6"
    }
   },
   "source": [
    "This section: load libraries and set up a log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "init_cell": true,
    "nbpresent": {
     "id": "c0035fc6-ff1d-44d8-a3fd-b4c08f53be71"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Python modules\n",
    "from datetime import datetime, date, timedelta, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import json\n",
    "import sqlite3\n",
    "import yaml\n",
    "import itertools\n",
    "import os\n",
    "import pytz\n",
    "import hashlib\n",
    "from shutil import copyfile\n",
    "\n",
    "# Scripts from household repository package\n",
    "from household.download import download\n",
    "from household.read import read\n",
    "from household.validation import validate\n",
    "from household.imputation import make_equidistant\n",
    "from household.imputation import find_nan\n",
    "from household.imputation import resample_markers\n",
    "\n",
    "# Reload modules with execution of any code, to avoid having to restart\n",
    "# the kernel after editing timeseries_scripts\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "households_yaml_path = 'input/households.yml'\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('log')\n",
    "# For more detailed logging messages, replace 'INFO' with 'DEBUG'\n",
    "# (May slow down computation).\n",
    "#logger.setLevel('DEBUG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Set version number and recent changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing this script till the end will create a new version of the data package.\n",
    "The Version number specifies the local directory for the data <br>\n",
    "We include a note on what has been changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "version = '2017-06-30'\n",
    "data_path = os.path.join(version, 'original_data')\n",
    "changes = '''Initial upload'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Select timerange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the time range to read and process data. <br>\n",
    "*Default: all data.*\n",
    "\n",
    "Type `None` to process all available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_from_user = None  # i.e. date(2016, 1, 1)\n",
    "end_from_user = None  # i.e. date(2016, 1, 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Select download source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data can be downloaded as a zip file from the OPSD Server. To do this, specify an archive version to use, that has been cached on the OPSD server as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "archive_version = '2017-06-30' # i.e. '2017-06-30'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section: download raw data to process.\n",
    "\n",
    "If the original data does not exist, it will be downloaded from the OPSD Server and extracted in a local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "format": "row",
    "nbpresent": {
     "id": "0c1eb987-6d5f-4e3d-9248-df80b9f37a49"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "download(version=archive_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d0b353e7-179d-4556-bdd2-270192c830fb"
    }
   },
   "source": [
    "# 4. Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d223207e-2b0e-49ec-8bf6-af2969ee5b28"
    }
   },
   "source": [
    "This section: Read each downloaded file into a pandas-DataFrame and merge data from different sources if it has the same time resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1300cbb2-efde-4844-b08c-fed092023e38"
    }
   },
   "source": [
    "Set the title of the rows at the top of the data used to store metadata internally. The order of this list determines the order of the levels in the resulting output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "format": "row",
    "init_cell": true,
    "nbpresent": {
     "id": "4dc92cc3-c01d-4c83-9252-80958edbe0f9"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "headers = ['project', 'region', 'household', 'type', 'feed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an empty dictionary to be populated by the read function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "init_cell": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_households = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Select household subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, specify a subset of households to process. <br>\n",
    "The next cell prints the available sources and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(households_yaml_path, 'r') as f:\n",
    "    households = yaml.load(f.read())\n",
    "for k, v in households.items():\n",
    "    print(yaml.dump({k: list(v['feeds'].keys())}, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy from its output and paste to following cell to get the right format.\n",
    "\n",
    "Type `subset = None` to include all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset = yaml.load('''\n",
    "    insert_household_here:\n",
    "    - insert_feed1_here\n",
    "    - insert_feed2_here\n",
    "    more_households_here:\n",
    "    - more_feeds_here\n",
    "    ''')\n",
    "\n",
    "subset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now eliminate households and feeds not in subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if subset:  # eliminate households and feeds not in subset\n",
    "    households = {household_name: {k: v\n",
    "                                   for k, v in households[household_name].items()\n",
    "                                   for k, v in households[household_name]['feeds'].items()\n",
    "                                   if k in feed_list}\n",
    "                  for household_name, feed_list in subset.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Reading loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through households and feeds to do the reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each source in the household dictionary\n",
    "for household_name, household_dict in households.items():\n",
    "    df = read(household_name, household_dict['dir'], household_dict['feeds'], \n",
    "              household_dict['project'], household_dict['region'], household_dict['type'], headers, \n",
    "              start_from_user=start_from_user,\n",
    "              end_from_user=end_from_user)\n",
    "    \n",
    "    data_households[household_name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Save raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the DataFrames created by the read function to disk. This way you have the raw data to fall back to if something goes wrong in the remainder of this notebook without having to repeat the previos steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for household_name, household_dict in households.items():\n",
    "    data_households[household_name].to_pickle('raw_'+household_dict['dir']+'.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the DataFrames saved above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_households = {}\n",
    "for household_name, household_dict in households.items():\n",
    "    data_households[household_name] = pd.read_pickle('raw_'+household_dict['dir']+'.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section: validate energy data and fix wrongly measured data points. Handle missing data and aggregation to regular and 15 minute intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Validate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the raw data being the unvalidated measurements of a research prototype under development, certain steps should be taken, to remove clearly incorrect measured data points or react to other events, such as the counter reset of an energy meter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for household_name, household_dict in households.items():\n",
    "    data_households[household_name] = validate(data_households[household_name], household_name, \n",
    "                                               household_dict['feeds'], household_dict['adjustments'], \n",
    "                                               headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5.2 Aggregate Index equidistantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the acquired household data comes in irregular time intervals and is highly unsynchronized for different data feeds. \n",
    "\n",
    "To improve readability and comparability, regular intervals will be assumed and interpolated according to existing values. Gaps greater than 15 minutes will be left untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sets = {}\n",
    "for household_name, household_dict in households.items():\n",
    "    for res_dict in household_dict['resolutions']:\n",
    "        res_key = res_dict['key']\n",
    "        \n",
    "        df = make_equidistant(data_households[household_name], household_name, \n",
    "                              res_key, res_dict.get('seconds'), \n",
    "                              res_dict.get('start'), res_dict.get('end'), \n",
    "                              household_dict['feeds'])\n",
    "        \n",
    "        if not res_key in data_sets:\n",
    "            data_sets[res_key] = df\n",
    "        elif not df.empty:\n",
    "            data_sets[res_key] = (\n",
    "                data_sets[res_key]\n",
    "            ).combine_first(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save/Load the equidistant data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for res_key, df in data_sets.items():\n",
    "    df.to_pickle('fixed_'+res_key+'.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sets = {}\n",
    "data_sets['1min'] = pd.read_pickle('fixed_1min.pickle')\n",
    "data_sets['3min'] = pd.read_pickle('fixed_3min.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Missing Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch missing data. At this stage, only small gaps (up to 2 hours) are filled by linear interpolation. This catched most of the missing data due to daylight savings time transitions or failed radio transmissions, while leaving bigger gaps untouched.\n",
    "\n",
    "The exact locations of missing data are stored in the `nan_table` DataFrames.\n",
    "\n",
    "Where data has been interpolated, it is marked in a new column `comment`. For eaxample the comment `residential_004_pv;` means that in the original data, there is a gap in the solar generation timeseries from the Resident 4 in the time period where the marker appears.\n",
    "\n",
    "Patch the datasets and display the location of missing Data in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nan_sets = {}\n",
    "for res_key, df in data_sets.items():\n",
    "    data_sets[res_key], nan_table = find_nan(df, headers, res_key, patch=True)\n",
    "    \n",
    "    nan_sets[res_key] = nan_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute this to see an example of where the data has been patched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sets['3min'][data_sets['3min']['interpolated_values'].notnull()].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sets['1min'][data_sets['1min']['interpolated_values'].notnull()].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the table of regions of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nan_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can export the NaN-tables to Excel in order to inspect where there are NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('NaN_table.xlsx')\n",
    "for res_key, df in nan_sets.items():\n",
    "    df.to_excel(writer, res_key)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save/Load the patched data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for res_key, df in data_sets.items():\n",
    "    df.to_pickle('patched_'+res_key+'.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sets = {}\n",
    "data_sets['1min'] = pd.read_pickle('patched_1min.pickle')\n",
    "data_sets['3min'] = pd.read_pickle('patched_3min.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5.4 Aggregate hourly and 15min interval data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As some data comes in 1-minute, other (older series) in 3-minute intervals, a harmonization can be done. With most billing intervals being at 15-minute and 60-minute ranges, a resampling to those intervals can be done to improve the overview over the data.\n",
    "\n",
    "The marker column is resampled separately in such a way that all information on where data has been interpolated is preserved.\n",
    "\n",
    "To do this, first combine the patched resolutions again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_full = None\n",
    "for res_key, df in data_sets.items():\n",
    "    if data_full is None:\n",
    "        data_full = df\n",
    "    elif not df.empty:\n",
    "        data_full = data_full.combine_first(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, condense the marker column to both 15 and 60 minutes resolution, resample the series and update the condensed markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_15 = data_full.index[0].replace(minute=data_full.index[0].minute + (15 - data_full.index[0].minute % 15), second=0)\n",
    "end_15 = data_full.index[-1].replace(minute=data_full.index[-1].minute - (data_full.index[-1].minute % 15), second=0)\n",
    "index_15 = pd.DatetimeIndex(start=start_15, end=end_15, freq='15min')\n",
    "marker_15 = data_full['interpolated_values'].groupby(\n",
    "    pd.Grouper(freq='15min', closed='left', label='left')\n",
    "    ).agg(resample_markers).reindex(index_15)\n",
    "\n",
    "data_sets['15min'] = data_full.resample('15min').last()\n",
    "data_sets['15min']['interpolated_values'] = marker_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_60 = data_full.index[0].replace(minute=0, second=0)\n",
    "end_60 = data_full.index[-1].replace(minute=0, second=0)\n",
    "index_60 = pd.DatetimeIndex(start=start_60, end=end_60, freq='60min')\n",
    "marker_60 = data_full['interpolated_values'].groupby(\n",
    "    pd.Grouper(freq='60min', closed='left', label='left')\n",
    "    ).agg(resample_markers).reindex(index_60)\n",
    "\n",
    "data_sets['60min'] = data_full.resample('60min').last()\n",
    "data_sets['60min']['interpolated_values'] = marker_60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Insert a column with Central European Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index column of th data sets defines the start of the timeperiod represented by each row of that data set in **UTC** time. We include an additional column for the **CE(S)T** Central European (Summer-) Time, as this might help aligning the output data with other data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "info_cols = {'utc': 'utc_timestamp',\n",
    "             'cet': 'cet_cest_timestamp',\n",
    "             'marker': 'interpolated_values'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for res_key, df in data_sets.items():\n",
    "    if df.empty:\n",
    "        continue\n",
    "    df.index.rename(info_cols['utc'], inplace=True)\n",
    "    df.insert(0, info_cols['cet'],\n",
    "              df.index.tz_localize('UTC').tz_convert('Europe/Brussels'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a final savepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for res_key, df in data_sets.items():\n",
    "    df.to_pickle('final_'+res_key+'.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sets = {}\n",
    "data_sets['1min'] = pd.read_pickle('final_1min.pickle')\n",
    "data_sets['3min'] = pd.read_pickle('final_3min.pickle')\n",
    "data_sets['15min'] = pd.read_pickle('final_15min.pickle')\n",
    "data_sets['60min'] = pd.read_pickle('final_60min.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Initialisation Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "nbpresent": {
   "slides": {
    "f6b300bf-88b5-4dea-951e-c926a9ea8287": {
     "id": "f6b300bf-88b5-4dea-951e-c926a9ea8287",
     "prev": "f96dd4bc-93a6-4014-b85f-a43061cf5688",
     "regions": {
      "dc486e18-7547-4610-99c0-55dfb5553f62": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "c0035fc6-ff1d-44d8-a3fd-b4c08f53be71",
        "part": "source"
       },
       "id": "dc486e18-7547-4610-99c0-55dfb5553f62"
      }
     }
    },
    "f96dd4bc-93a6-4014-b85f-a43061cf5688": {
     "id": "f96dd4bc-93a6-4014-b85f-a43061cf5688",
     "prev": null,
     "regions": {
      "657c3ad3-2fcf-4c8e-a527-de3d0a46fa4e": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "1562965a-7d74-4c1c-8251-4d82847f294a",
        "part": "source"
       },
       "id": "657c3ad3-2fcf-4c8e-a527-de3d0a46fa4e"
      }
     }
    }
   },
   "themes": {}
  },
  "notify_time": "10"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
